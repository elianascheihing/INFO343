{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia en LDA\n",
    "El problema de inferencia a resolver para usar LDA es el cálculo de la distribución a posteriori:\n",
    "$$p(\\theta, {\\bf z} \\mid {\\bf w},\\alpha,\\beta) = \\frac{p(\\theta, {\\bf z} , {\\bf w}\\mid \\alpha,\\beta)}{p({\\bf w}\\mid \\alpha,\\beta)}$$\n",
    "dónde \n",
    "$$p(\\theta, {\\bf z}, {\\bf w}\\mid \\alpha,\\beta)= p(\\theta \\mid \\alpha) \\prod_{n=1}^N p(z_n\\mid \\theta) p(w_n\\mid z_n,\\beta)$$\n",
    "siendo $N$ el número de palabras en el documento,\n",
    "$${\\bf z} = (z_1,...,z_N),\\qquad {\\bf w} = (w_1,...,w_N)$$\n",
    "$$z_n = (0,0,...,1,...,0) \\in \\{0,1\\}^K$$ con \n",
    "$z_n^i =  1$ si la n-ésima palabra es asignada al tópico i, \n",
    "0 si no.\n",
    "$$\\,$$\n",
    "$$z_n \\sim Multinomial(\\theta)$$ entonces\n",
    "$$p(z_n \\mid \\theta) = \\frac{1!}{0!...1!...0!}\\prod_{i=1}^K \\theta_i^{z_n^i\n",
    "}= \\theta_i \\qquad cuando \\qquad z_n^i = 1$$\n",
    "De manera equivalente se define\n",
    "$$w_n = (0,0,...,1,...,0) \\in \\{0,1\\}^V$$ con \n",
    "$w_n^j =  1$ si la n-ésima palabra corresponde a la j-ésima palabra del vocabulario, \n",
    "0 si no.\n",
    "$$\\,$$\n",
    "Por otra parte, se define \n",
    "$$\\beta_{ij} = p(w_n^j = 1 \\mid z_n^i =1),\\qquad i = 1,...K, j= 1,...,V$$\n",
    "\n",
    "Así,si $z_n^i =1$ entonces $w_n \\sim Multinomial(\\beta_i)$, es decir:\n",
    "\n",
    "$$p(w_n \\mid z_n,\\beta) = \\frac{1!}{0!...1!...0!}\\prod_{j=1}^V \\beta_{ij}^{w_n^j\n",
    "}= \\beta_{ij} \\qquad \\text{cuando} \\qquad w_n^j = 1 y \\qquad z_n^i =1$$\n",
    "\n",
    "De manera que:\n",
    "\n",
    "$$p(\\theta, {\\bf z}, {\\bf w}\\mid \\alpha,\\beta)= p(\\theta \\mid \\alpha) \\prod_{n=1}^N \\left(\\prod_{i=1}^K \\left[\\theta_i \\prod_{j=1}^V \\beta_{ij}^{w_n^j}\\right]^{z_n^i}\\right)$$\n",
    "\n",
    "Al integrar sobre todos los valores posibles de $\\theta$ y sumar sobre todos los valores posibles de ${\\bf z}$ se obtiene:\n",
    "$$p({\\bf w}\\mid \\alpha,\\beta)= \\int_{\\Theta} \\sum_{\\bf z} p(\\theta \\mid \\alpha) \\prod_{n=1}^N \\left(\\prod_{i=1}^K \\left[\\theta_i \\prod_{j=1}^V \\beta_{ij}^{w_n^j}\\right]^{z_n^i}\\right)d\\theta$$\n",
    "Los valores posibles de ${\\bf z} = (z_1,...,z_N)$ dependen de los $z_i$ que independientes uno de otros toman $K$ valores posibles. En efecto cada $z_n$ puede tomar los K valores siguientes:\n",
    "$$(1,0,\\cdots,0), (0,1,\\cdots,0),\\cdots (0,0,\\cdots,1)$$\n",
    "Sea entonces $z_n$ tal que $z_n^i = 1$ y $z_n^j =0 \\forall j \\neq i, i=1,\\cdots, K$ entonces:\n",
    "$$\\left(\\prod_{i=1}^K \\left[\\theta_i \\prod_{j=1}^V \\beta_{ij}^{w_n^j}\\right]^{z_n^i}\\right) = \\left[\\theta_i \\prod_{j=1}^V \\beta_{ij}^{w_n^j}\\right]$$\n",
    "Y asi:\n",
    "$$p({\\bf w}\\mid \\alpha,\\beta)= \\int_{\\Theta}  p(\\theta \\mid \\alpha) \\prod_{n=1}^N \\left(\\sum_{i=1}^K \\left[\\theta_i\\prod_{j=1}^V \\beta_{ij}^{w_n^j}\\right]\\right )d\\theta$$\n",
    "\n",
    "Como $\\theta \\sim Dirichlet(\\alpha)$ entonces\n",
    "$$p({\\bf w}\\mid \\alpha,\\beta)= \\frac{\\Gamma(\\sum_{i=1}^K \\alpha_i)}{\\prod_{i=1}^K \\Gamma(\\alpha_i)} \\int_{\\Theta} \\left(\\prod_{i=1}^K \\theta^{\\alpha_i -1}\\right) \\left( \\prod_{n=1}^N \\left(\\sum_{i=1}^K \\prod_{j=1}^V (\\theta_i\\beta_{ij})^{w_n^j}\\right)\\right )d\\theta$$\n",
    "que es intratable debido al acoplamiento entre $\\beta$ y $\\theta$ en la suma de tópicos latentes.\\\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia Variacional\n",
    "(Ver <a href= \"https://drive.google.com/open?id=1BobImO3192hifZPLXowd14gryVAUzBPW\"> Latent Dirichlet Allocation (LDA) </a>)\n",
    "$$\\,$$\n",
    "La idea básica de la inferencia variacional es definir una familia de funciones parametrizadas que acoten inferiormente a la distribución a posteriori, y entonces la inferencia se transforma en un problema de maximización de esa familia. \n",
    "\n",
    "Para ello se propone una familia de distribuciones a priori en que se encuentran desacopladas $\\theta$ y ${\\bf z}$, es decir\n",
    "$$ \\theta \\sim Dirichlet(\\gamma)$$\n",
    "y\n",
    "$$ {\\bf z} \\sim Multinomial(\\phi)$$\n",
    "Así, \n",
    "$$q(\\theta,{\\bf z} \\mid \\gamma,\\phi) = q(\\theta \\mid \\gamma)\\prod_{n=1}^N q(z_n \\mid \\phi_n)$$\n",
    "permite definir una familia de funciones que acota inferiormente el logaritmo de la distribución predictiva o evidencia.\n",
    "En efecto\n",
    "$$log p({\\bf w} \\mid \\alpha,\\beta) = log\\int_\\Theta \\sum_{\\bf z} p(\\theta, {\\bf z}, {\\bf w}\\mid \\alpha,\\beta)d\\theta$$\n",
    "$$ = log\\int_\\Theta \\sum_{\\bf z} \\frac{p(\\theta, {\\bf z}, {\\bf w}\\mid \\alpha,\\beta)}{q(\\theta,{\\bf z})} q(\\theta,{\\bf z})d\\theta$$\n",
    "$$ = log \\mathop{\\mathbb{E}_q}\\left[\\frac{p(\\cdot, \\cdot, {\\bf w}\\mid \\alpha,\\beta)}{q(\\cdot,\\cdot)}\\right] $$\n",
    "por la desigualdad de Jensen para el caso de funciones cóncavas (caso del logaritmo):\n",
    "$$\\geq \\mathop{\\mathbb{E}_q} log\\left[\\frac{p(\\cdot, \\cdot, {\\bf w}\\mid \\alpha,\\beta)}{q(\\cdot,\\cdot)}\\right] $$\n",
    "$$ = \\mathop{\\mathbb{E}_q}\\left[log p(\\cdot, \\cdot, {\\bf w}\\mid \\alpha,\\beta)\\right] - \\mathop{\\mathbb{E}_q}\\left[log q(\\cdot,\\cdot)\\right] $$\n",
    "Antes de proseguir, vamos a calcular la divergencia de Kullback-Leibler entre la distribución a posteriori y q:\n",
    "$$D_{KL}(q \\mid p) = \\mathop{\\mathbb{E}_q}\\left[log \\frac{q(\\cdot,\\cdot)}{p(\\cdot, \\cdot \\mid {\\bf w} ,\\alpha,\\beta)}\\right] $$\n",
    "$$\\,$$\n",
    "$$= \\mathop{\\mathbb{E}_q}\\left[log q(\\cdot,\\cdot)\\right] -  \\mathop{\\mathbb{E}_q}\\left[log p(\\cdot, \\cdot\\mid {\\bf w}, \\alpha,\\beta)\\right] $$\n",
    "$$\\,$$\n",
    "$$= \\mathop{\\mathbb{E}_q}\\left[log q(\\cdot,\\cdot)\\right] -  \\mathop{\\mathbb{E}_q}\\left[log \\frac{p(\\cdot, \\cdot , {\\bf w}\\mid \\alpha,\\beta)}{p({\\bf w}\\mid \\alpha,\\beta)}\\right] $$\n",
    "$$\\,$$\n",
    "$$= \\mathop{\\mathbb{E}_q}\\left[log q(\\cdot,\\cdot)\\right] -  \\mathop{\\mathbb{E}_q}\\left[log p(\\cdot, \\cdot , {\\bf w}\\mid \\alpha,\\beta)\\right] + \\mathop{\\mathbb{E}_q}\\left[log p({\\bf w}\\mid \\alpha,\\beta)\\right] $$\n",
    "$$\\,$$\n",
    "$$= \\mathop{\\mathbb{E}_q}\\left[log q(\\cdot,\\cdot)\\right] -  \\mathop{\\mathbb{E}_q}\\left[log p(\\cdot, \\cdot , {\\bf w}\\mid \\alpha,\\beta)\\right] + log p({\\bf w}\\mid \\alpha,\\beta) $$\n",
    "$$\\,$$\n",
    "Es decir,\n",
    "si denominamos\n",
    "$$L(\\gamma,\\phi,\\alpha,\\beta) = \\mathop{\\mathbb{E}_q}\\left[log p(\\cdot, \\cdot, {\\bf w}\\mid \\alpha,\\beta)\\right] - \\mathop{\\mathbb{E}_q}\\left[log q(\\cdot,\\cdot\\mid \\gamma,\\phi)\\right]$$\n",
    "tenemos que\n",
    "$$log p({\\bf w}\\mid \\alpha,\\beta) = L(\\gamma,\\phi,\\alpha,\\beta) + D_{KL}(q \\mid p) $$\n",
    "Entonces, minimizar $D_{KL}(q \\mid p)$\n",
    "es equivalente a maximizar L.\n",
    "El problema de inferencia se transforma en el problema de maximización:\n",
    "$$\\mathop{max}_{\\gamma,\\phi} L(\\gamma,\\phi,\\alpha,\\beta)$$\n",
    "Ver en los anexos A3.1 y A3.2 del artículo en referencia los detalles de la resolución de este problema de maximización, asi como en A4 las explicaciones de como se utiliza la estrategia EM para encontrar los estimadores de Bayes empíricos  para $\\alpha$ y $\\beta$.\n",
    "$$\\,$$\n",
    "Posteriormente, en <a href=\"https://drive.google.com/file/d/1f7XoM6VlmnjL6YNX-VuuTYqRT85k_jJf/view?usp=sharing\"> Hoffman, Blei y Bach (2010) </a> se propone un nuevo algoritmo de inferencia variacional online para el modelo LDA, que es el que usa la biblioteca sklearn de Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia con Gibbs Sampling\n",
    "(Ver <a href= \"https://drive.google.com/file/d/1oSRRvG_W0UaH0l4o9f5ClEyWuGY1j-s0/view?usp=sharing\"> Gibbs sampling para Latent Dirichlet Allocation (LDA) </a>)\n",
    "$$\\,$$\n",
    "Para la formulación de la inferencia utilizando Gibbs sampling, es necesario considerar una distribución a priori Dirichlet sobre el parámetro $\\beta$ que representa la distribución de tópicos en cada documento. Tenemos entonces:\n",
    "\n",
    "$$w_n \\mid z_n, \\beta^{(z_n)} \\sim Multinomial(\\beta^{(z_n)})$$\n",
    "\n",
    "$$\\beta \\sim Dirichlet(\\eta)$$\n",
    "\n",
    "$$ z_n \\mid \\theta \\sim Multinomial(\\theta)$$\n",
    "\n",
    "$$ \\theta \\sim Dirichlet(\\alpha)$$\n",
    "\n",
    "Entonces\n",
    "$$p({\\bf w},{\\bf z} \\mid \\theta,\\beta) = p({\\bf w}  \\mid {\\bf z}, \\theta,\\beta)p({\\bf z} \\mid \\theta)$$\n",
    "de dónde\n",
    "$$\\int_{\\Theta} \\int_{\\Gamma} p({\\bf w},{\\bf z} \\mid \\theta,\\beta)p(\\beta)d\\beta p(\\theta) d\\theta = \\int_{\\Gamma} p({\\bf w}  \\mid {\\bf z}, \\beta)p(\\beta)d\\beta \\int_{\\Theta} p({\\bf z} \\mid \\theta)p(\\theta) d\\theta$$\n",
    "\n",
    "Si consideramos\n",
    "$$\\eta = (\\eta_1,...,\\eta_V) = (\\eta,...,\\eta)$$\n",
    "y\n",
    "$$\\alpha = (\\alpha_1,...,\\alpha_K) = (\\alpha,...,\\alpha)$$\n",
    "\n",
    "entonces podemos escribir:\n",
    "\n",
    "$$p({\\bf z}=(0,0,\\cdots,1,\\cdots,0)\\mid \\theta) = \\frac{1!}{0!\\cdots 1!\\cdots 0!}\\theta_1^0\\cdots\\theta_i^1\\cdots\\theta_K^0 = \\theta_i$$\n",
    "\n",
    "y\n",
    "$$p(\\theta) = \\frac{\\Gamma(K\\alpha)}{(\\Gamma(\\alpha))^K}\\prod_{i=1}^K \\theta_i^{\\alpha}$$\n",
    "\n",
    "De esta manera:\n",
    "\n",
    "$$p({\\bf z}) = \\int_{\\Theta} p({\\bf z} \\mid \\theta)p(\\theta) d\\theta$$\n",
    "$$= \\int_{\\Theta} \\prod_{d=1}^D \\prod_{i=1}^K p(z^{(d)}_{i}=1\\mid \\theta)p(\\theta) d\\theta$$\n",
    "\n",
    "Sea $n_i^{(d)}$ es el número de veces que una palabra del documento fue asignada al tópico $i$, entonces:\n",
    "\n",
    "$$= \\left(\\frac{\\Gamma(K\\alpha)}{(\\Gamma(\\alpha))^K}\\right)^D \\, \\int_{\\Theta} \\prod_{i=1}^K \\theta_i^{n_i^{(d)}} \\theta_i^{\\alpha}d(\\theta)$$\n",
    "\n",
    "$$= \\left(\\frac{\\Gamma(K\\alpha)}{(\\Gamma(\\alpha))^K}\\right)^D \\prod_{d=1}^D \\prod_{i=1}^K \\frac{\\Gamma(n_i^{(d)} + \\alpha)}{\\Gamma(n_{\\cdot}^{(d)} + K\\alpha)}$$\n",
    "\n",
    "$$p({\\bf w} \\mid {\\bf z}) = \\int_{\\Gamma} p({\\bf w}  \\mid {\\bf z}, \\beta)p(\\beta)d\\beta$$\n",
    "$$= \\left(\\frac{\\Gamma(V\\eta)}{(\\Gamma(\\eta))^V}\\right)^K \\prod_{i=1}^K\n",
    "\\prod_{w \\in W} \\frac{\\Gamma(n_i^{(w)} + \\eta)}{\\Gamma(n_i^{(\\cdot)} + K\\eta)}$$\n",
    "dónde $n_i^{(w)}$ es el número de veces que una palabra $w$ es asignada al tópico $i$ en el vector de asignaciones ${\\bf z}$.\n",
    "Con estas especificaciones, el cálculo de la distribución a posteriori:\n",
    "\n",
    "$$p({\\bf z} \\mid {\\bf w}) =\\frac{p({\\bf w} \\mid {\\bf z})p({\\bf z})}{\\sum_{\\bf z} p({\\bf w} \\mid {\\bf z})p({\\bf z})}$$\n",
    "\n",
    "es intratable (la suma sobre ${\\bf z}$ es del orden de $V^N$).\n",
    "Entonces se considera el uso de Gibbs sampling aplicando el algoritmo a \n",
    "$$p(z_j \\mid z_{-j} , {\\bf w})$$\n",
    "donde $z_j$ es el tópico asignado a la j-ésima palabra y $z_{-j}$ son los tópicos asignados a las demás palabras del documento. De las propiedades de las distribuciones naturales conjugadas, se tiene que: \n",
    "$$p(z_j \\mid z_{-j} , {\\bf w}) \\propto \\frac{n_{-j,i}^{(w_j)} + \\eta}{n_{-j,i}^{(\\cdot)} + V\\eta} \\frac{n_{-j,i}^{(d_j)} + \\alpha}{n_{-j,\\cdot}^{(d_j)} + K\\alpha}$$\n",
    "donde\n",
    "$n_{-j,i}^{(w_j)}$ es el número de veces que $w_j$ (aparte de la j-ésima palabra) aparece en el corpus asignada al tópico i.\n",
    "$$\\,$$\n",
    "$n_{-j,i}^{(d_j)}$ es el número de palabras en $d_j$ (excepto la j-ésima palabra)  asignada al tópico i.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de Calidad de la Estructura de Tópicos\n",
    "\n",
    "Varios niveles de evaluación:\n",
    "\n",
    "¿El método logró capturar la estructura oculta del dataset?\n",
    "\n",
    "¿Se entienden los tópicos?\n",
    "\n",
    "¿Son coherentes los tópicos?\n",
    "\n",
    "¿El modelo nos permite analizar lo que queremos analizar? \n",
    "(requiere explicitar nuestros objetivos a priori)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplejidad\n",
    "Esta medida se considera intrínseca y proviene del modelamiento probabilista realizado. Su expresión es:\n",
    "\n",
    "$$perplejidad(D_{test}) = exp\\left\\{-\\frac{\\prod_{d=1}^D log(p(w_d))}{\\sum_{d=1}^D N_d}\\right\\}$$\n",
    "\n",
    "Para utilizarla se debe considerar un conjunto de entrenamiento y otro de test. De manera que la perplejidad se interpreta como una medida de robustez de la estructura de tópicos definida. \n",
    "Es inversamente proporcional a la verosimilitud de los nuevos datos. A menor valor de perplejidad mas robusto el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de coherencia:\n",
    "Probabilidad condicional que dos palabras aparezcan juntas \n",
    "(idealmente en un dataset de referencia)\n",
    "\n",
    "Existen muchas métricas de coherencia. Como por ejemplo:\n",
    "\n",
    "**Coherencia UMass**, propuesta por por Minmo et al(2011):\n",
    "\n",
    "$$C_{umass}= \\frac{2}{N(N-1)}\\sum_{i=2}^N \\sum_{j=1}^{i-1} log \\frac{p(w_i,w_j)}{p(w_j)}$$\n",
    "\n",
    "Para cada tópico se calcula su coherencia considerando las N top words del mismo. En este caso, se utilizan las probabilidades calculadas a partir del corpus en estudio.\n",
    "\n",
    "**Coherencia UCI:**\n",
    "\n",
    "$$C_{UCI}= \\frac{2}{N(N-1)}\\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} PMI(w_i,w_j)$$\n",
    "\n",
    "donde $$PMI(w_i,w_j)=  log \\frac{p(w_i,w_j)+\\epsilon}{p(w_i)p(w_j)}$$\n",
    "que se denomina *información mutua puntual*\n",
    "\n",
    "En este caso las probabilidades conjuntas y marginales son obtenidas de Wikipedia, como corpus externo. \n",
    "\n",
    "**Coherencia NPMI** propuesta por Aletras y Stevenson (2013):\n",
    "\n",
    "En este caso en lugar de considerar todas las probabilidades conjuntas, se considera un por cada top word $w_i$, un vector de las palabras adyacentes $v_i$ de manera que a la j-ésima componente de ese vector se la asocia la medida PMI normalizada:\n",
    "\n",
    "$$v_{ij} = NMPI(w_i,w_j)^\\gamma= \\left( \\frac{log \\frac{p(w_i,w_j)+\\epsilon}{p(w_i)p(w_j)}}{-log(p(w_i,w_j)+\\epsilon)}\\right)^{\\gamma}\n",
    "$$\n",
    "\n",
    "En este caso las probabilidades conjuntas y marginales también son obtenidas de Wikipedia, como corpus externo. \n",
    "Para medir la correlación entre los vectores construidos se pueden usar medidas como coseno, Dice, o Jacquard.\n",
    "\n",
    "\n",
    "**Coherencia V** propuesta por Röder et al. (2015) es una combinación de la coherencia NPMI con la medida coseno entre vectores vecinos. \n",
    "\n",
    "Un completo análisis de medidas coherencia se pueden encontrar en <a html=\"http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\"> Röder et al. (2015) - Exploring the space of topic coherence measures </a>\n",
    "\n",
    "En general:\n",
    "\n",
    "            →  no hay una métrica universal\n",
    "            \n",
    "            → la interpretacion del valor de la métrica es compleja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfoque “humano”: método por intrusión de palabras (Lau et al., 2014)\n",
    "\n",
    "\n",
    "Se inserta una palabra intrusiva en un tópico\n",
    "Se mide el acuerdo entre jueces humanos al momento de detectar la palabra intrusiva\n",
    "\n",
    "¿Cuál es tópico más coherente?\n",
    "\n",
    "    T1 = [perro, gato, caballo, manzana, gallina]\n",
    "\n",
    "    T2 = [gato, aeropuerto, manzana, seguridad, mañana]\n",
    "\n",
    "\n",
    "Los modelos de tópicos son bastante complejos de evaluar\n",
    "\n",
    "Las evaluaciones humanas toman tiempo y son bastante subjetivas\n",
    "\n",
    "Las evaluaciones automáticas no son universales y complejas de interpretar\n",
    "\n",
    "→ No existe un buen modelo universal, a menudo depende de lo que queremos hacer con el modelo (importante de explicitarlo antes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
